{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.4\n",
      "0.56.4\n",
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 2070 SUPER'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-1fab6b59-6c9e-c97a-a845-a4abc6279485\n",
      "                                Watchdog: Enabled\n",
      "                            Compute Mode: WDDM\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import numpy as np\n",
    " import numba\n",
    " from numba import cuda\n",
    " \n",
    " print(np.__version__)\n",
    " print(numba.__version__)\n",
    " \n",
    " cuda.detect()\n",
    " \n",
    " # 1.21.6\n",
    " # 0.55.2\n",
    " #\n",
    " # Found 1 CUDA devices\n",
    " # id 0             b'Tesla T4'                              [SUPPORTED]\n",
    " #                       Compute Capability: 7.5\n",
    " #                            PCI Device ID: 4\n",
    " #                               PCI Bus ID: 0\n",
    " #                                     UUID: GPU-e0b8547a-62e9-2ea2-44f6-9cd43bf7472d\n",
    " #                                 Watchdog: Disabled\n",
    " #              FP32/FP64 Performance Ratio: 32\n",
    " # Summary:\n",
    " # 1/1 devices are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU process time: 1.9570248126983643\n",
      "GPU process time: 0.13796734809875488\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from numba import cuda\n",
    "import time\n",
    "import math\n",
    "\n",
    "# GPU function\n",
    "@cuda.jit()\n",
    "def process_gpu(img):\n",
    "    tx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    ty = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    for channel in range(3):\n",
    "        color = img[tx, ty][channel] * 2.0 + 30\n",
    "        if color > 255:\n",
    "            img[tx, ty][channel] = 255\n",
    "        elif color < 0:\n",
    "            img[tx, ty][channel] = 0\n",
    "        else:\n",
    "            img[tx, ty][channel] = color\n",
    "\n",
    "\n",
    "# CPU function\n",
    "def process_cpu(img, dst):\n",
    "    height, width, channels = img.shape\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            for c in range(channels):\n",
    "                color = img[h, w][c] * 2.0 + 30\n",
    "                if color > 255:\n",
    "                    dst[h, w][c] = 255\n",
    "                elif color < 0:\n",
    "                    dst[h, w][c] = 0\n",
    "                else:\n",
    "                    dst[h, w][c] = color\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img = cv2.imread(\"./Figure_1.png\")\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    dst_cpu = img.copy()\n",
    "    start_cpu = time.time()\n",
    "    process_cpu(img, dst_cpu)\n",
    "    end_cpu = time.time()\n",
    "    time_cpu = (end_cpu - start_cpu)\n",
    "    print(\"CPU process time: \" + str(time_cpu))\n",
    "\n",
    "    ##GPU function\n",
    "    dImg = cuda.to_device(img)\n",
    "    threadsperblock = (32, 32)\n",
    "    blockspergrid_x = int(math.ceil(height / threadsperblock[0]))\n",
    "    blockspergrid_y = int(math.ceil(width / threadsperblock[1]))\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    cuda.synchronize()\n",
    "    start_gpu = time.time()\n",
    "    process_gpu[blockspergrid, threadsperblock](dImg)\n",
    "    end_gpu = time.time()\n",
    "    cuda.synchronize()\n",
    "    time_gpu = (end_gpu - start_gpu)\n",
    "    print(\"GPU process time: \" + str(time_gpu))\n",
    "    dst_gpu = dImg.copy_to_host()\n",
    "\n",
    "    # save\n",
    "    cv2.imwrite(\"result_cpu.jpg\", dst_cpu)\n",
    "    cv2.imwrite(\"result_gpu.jpg\", dst_gpu)\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.4\n",
      "0.56.4\n",
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 2070 SUPER'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-1fab6b59-6c9e-c97a-a845-a4abc6279485\n",
      "                                Watchdog: Enabled\n",
      "                            Compute Mode: WDDM\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "2.0 + 7.0 = 9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    " import numpy as np\n",
    " import numba\n",
    " from numba import cuda\n",
    " \n",
    " print(np.__version__)\n",
    " print(numba.__version__)\n",
    " \n",
    " cuda.detect()\n",
    " \n",
    "  # Example 1.1: Add scalars\n",
    " @cuda.jit\n",
    " def add_scalars(a, b, c):\n",
    "     c[0] = a + b\n",
    " \n",
    " dev_c = cuda.device_array((1,), np.float32)\n",
    " \n",
    " add_scalars[1, 1024](2.0, 7.0, dev_c)\n",
    " \n",
    " c = dev_c.copy_to_host()\n",
    " print(f\"2.0 + 7.0 = {c[0]}\")\n",
    " #  2.0 + 7.0 = 9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numba.cuda.stubs.blockIdx"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.gridDim.x\n",
    "cuda.blockDim.x\n",
    "cuda.blockIdx\n",
    "cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18. 20. 22. 24. 26. 28. 30. 32. 34.\n",
      " 36. 38.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "d:\\ProgramData\\miniconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Example 1.2: Add arrays\n",
    "@cuda.jit\n",
    "def add_array(a, b, c):\n",
    "    i = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    if i < a.size:\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "N = 20\n",
    "a = np.arange(N, dtype=np.float32)\n",
    "b = np.arange(N, dtype=np.float32)\n",
    "dev_c = cuda.device_array_like(a)\n",
    "\n",
    "add_array[4, 8](a, b, dev_c)\n",
    "import time \n",
    "start_time = time.time()\n",
    "c = dev_c.copy_to_host()\n",
    "end_time = time.time()\n",
    "print(c)\n",
    "print(\"total time {}s\".format(end_time - start_time))\n",
    "#  [ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18. 20. 22. 24. 26. 28. 30. 32. 34. 36. 38.]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
